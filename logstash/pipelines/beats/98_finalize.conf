filter {

    if (![event][hash]) {

      # create a repeatable fingerprint for document ID into event.hash

      if ([event][original]) {
        fingerprint {
          id => "fingerprint_malcolm_beats_event_original"
          source => [ "[host][name]",
                      "[event][original]",
                      "[@timestamp]" ]
          concatenate_sources => true
          # uses event.hash
          ecs_compatibility => "v8"
          method => "MURMUR3_128"
          base64encode => true
        }

      } else {
        # at this point we should have already assigned a hash (or had event.original
        # to hash). This generally should not happen so if it is reexamine where we're not
        # doing the work we should be.
        fingerprint {
          id => "fingerprint_malcolm_beats_unknown"
          source => [ "[host][name]",
                      "[event][module]",
                      "[event][dataset]",
                      "[agent][type]",
                      "[input][type]",
                      "[@timestamp]" ]
          concatenate_sources => true
          # uses event.hash
          ecs_compatibility => "v8"
          method => "MURMUR3_128"
          base64encode => true
        }

      } #if [event][original] /else
    } # ![event][hash]

    # arkime doesn't like / in the record ID
    mutate { id => "mutate_beats_gsub_event_hash_urlsafe"
             gsub => [ "[event][hash]", "/", "_",
                       "[event][hash]", "\+", "-",
                       "[event][hash]", "=+", "" ] }

    # this identifies which node the log came from
    if ([host][name]) {
      mutate { id => "mutate_beats_add_field_host_name_node"
               add_field => { "[node]" => "%{[host][name]}" } }
    } else {
      mutate { id => "mutate_beats_add_field_logstash_node"
               add_field => { "[node]" => "malcolm" } }
    }

    # trim path portion of originating log file
    if ([log][file][path]) { mutate { id => "mutate_beats_gsub_log_file_path_directory"
                                      gsub => [ "[log][file][path]", "^.*/", "" ] } }

    # remove some fields we don't need (or don't need anymore)
    mutate {
      id => "mutate_beats_remove_field_useless"
      remove_field => [
        "[beat]",
        "[agent][ephemeral_id]",
        "[agent][id]",
        "[agent][version]",
        "[log][offset]",
        "[log][source]",
        "[prospector]"
      ]
    }

    # event.provider
    if (![event][provider]) { mutate { id => "mutate_add_field_event_provider_beats"
                                       add_field => { "[event][provider]" => "malcolm" } } }

    # event.module
    if (![event][module]) { mutate { id => "mutate_add_field_event_module_beats"
                                     add_field => { "[event][module]" => "beats" } } }

    # event.ingested
    if (![event][ingested]) {
      ruby {
        id => "ruby_beats_event_ingested_now"
        init => "require 'time'"
        code => "event.set('[event][ingested]', Time.now.to_f)"
      }
      date {
        id => "date_beats_event_ingested_conv"
        match => [ "[event][ingested]", "UNIX" ]
        target => "[event][ingested]"
      }
    }

    # generate opensearch index name
    if (![@metadata][malcolm_opensearch_index]) {
      ruby {
        id => "ruby_resolve_beats_logs_index"
        path => "/usr/share/logstash/malcolm-ruby/format_index_string.rb"
        script_params => {
          "target" => "[@metadata][malcolm_opensearch_index]"
          "prefix_env" => "MALCOLM_OTHER_INDEX_PATTERN"
          "prefix_default" => "malcolm_beats_*"
          "suffix_env" => "MALCOLM_OTHER_INDEX_SUFFIX"
          "suffix_default" => "%{%y%m%d}"
          "midfix_fields" => [ "[event][module]", "[agent][type]", "[input][type]" ]
        }
      }
    }

}